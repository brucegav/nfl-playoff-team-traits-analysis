{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eebc61-8d02-4a0a-9233-ebe7e535add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nflreadpy as nfl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6f0cf-121d-48a8-8902-df0ef82b9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NFLREADPY_CACHE'] = 'filesystem'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb668f6-c972-46a6-b0ef-42e0813fd5b6",
   "metadata": {},
   "source": [
    "## Data Loading and Cleaning\n",
    "\n",
    "Data Loading\n",
    "-------------\n",
    "1. Load dataset of play-by-play data from nflreadpy for seasons 2011 to 2025\n",
    "2. Filter by season_type to include only play-by-play data from playoff games stored in playoff_master dataframe\n",
    "\n",
    "Data Cleaning\n",
    "-------------\n",
    "1. Investigate data in dataframe columns and determine what types of data needed for analysis/modeling\n",
    "2. Drop all columns that do not deal with game context, game situation.  We do not want the model to have access to any play result data.  Only context, game situation, and play environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295af21-9ce5-4976-91b0-1a8375f8dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = list(range(1999, 2026))\n",
    "\n",
    "df = nfl.load_pbp(seasons).to_pandas()\n",
    "\n",
    "playoff_master = df[df['season_type'] == 'POST'].copy()\n",
    "\n",
    "playoff_master = playoff_master.sort_values(by=['game_id', 'play_id']).reset_index(drop=True)\n",
    "\n",
    "print(f'playoff_master shape: {playoff_master.shape}')\n",
    "playoff_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2c69c-2179-4534-abcf-f08264a760db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features\n",
    "features = [\n",
    "    'yardline_100', 'game_seconds_remaining', 'down', 'ydstogo', 'play_type', 'shotgun', 'no_huddle', \n",
    "    'score_differential', 'posteam_timeouts_remaining', 'defteam_timeouts_remaining', 'roof', 'surface',\n",
    "    'temp', 'wind'\n",
    "]\n",
    "\n",
    "# define target\n",
    "target = 'success'\n",
    "\n",
    "model_df = playoff_master[['game_id', 'play_id'] + features + [target]].copy()\n",
    "\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7794ebd-7927-45d2-ae4f-1bc6a2101fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd231fa-76d0-4def-a16e-1d1c62ba44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## handle NaN values for temp and wind which appear to all be in instances of 'roof' == 'closed' or 'dome'\n",
    "# define indoor stadiums\n",
    "indoor = ['closed', 'dome']\n",
    "\n",
    "# set wind to 0 (no wind indoors) for all instances of 'closed' or 'dome'\n",
    "model_df.loc[model_df['roof'].isin(indoor), 'wind'] = 0\n",
    "\n",
    "# set temp to 70 (room temp) for all instances of 'closed' or 'dome'\n",
    "model_df.loc[model_df['roof'].isin(indoor), 'temp'] = 65\n",
    "\n",
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17691226-7bfa-4715-8bae-ae23f2bc8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "playoff_master[playoff_master['temp'].isnull()]['game_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771fe9a-cf1b-452c-acd6-bb0242790df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df[model_df['temp'].isnull()]['roof'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb3a8c-fc01-4b83-8dde-b721e2c8a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['game_id'] = playoff_master['game_id']\n",
    "\n",
    "model_df[model_df['temp'].isnull()]['game_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede379dd-cd40-4849-8c37-7e208d4621f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## manually enter temp and wind values for these 3 games \n",
    "model_df.loc[model_df['game_id'] == '2019_19_HOU_KC', ['temp', 'wind']] = [24, 6]\n",
    "\n",
    "model_df.loc[model_df['game_id'] == '2019_20_TEN_KC', ['temp', 'wind']] = [17, 10]\n",
    "\n",
    "model_df.loc[model_df['game_id'] == '2019_21_SF_KC', ['temp', 'wind']] = [62, 6]\n",
    "\n",
    "print(model_df[model_df['temp'].isnull()]['game_id'].unique())\n",
    "print(model_df[model_df['temp'].isnull()]['roof'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f3810-28e2-4b44-970a-bdaaa84241a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732512a-c746-47ab-ab18-887d6943ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter model_df for only scrimmage plays (passes and runs)\n",
    "model_df = model_df[model_df['play_type'].isin(['pass', 'run'])]\n",
    "\n",
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f490653-15b4-41c5-bb36-3a6514a7e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop all null in success and down columns\n",
    "## if a play has no success value, it is useless for training\n",
    "## if a play has no down value, this a lack of critical data for game situation\n",
    "model_df = model_df.dropna(subset=['success', 'down'])\n",
    "\n",
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829863c2-c340-4a95-9b3a-4344a6cb5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dummies for categorical columns (play_type, roof, surface), ignore game_id as that will be removed before modeling\n",
    "\n",
    "model_df = pd.get_dummies(model_df, columns=[\n",
    "    'play_type',\n",
    "    'roof',\n",
    "    'surface'\n",
    "], drop_first=True, dtype=int)\n",
    "\n",
    "model_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95ebdc-56c3-4598-99de-4f1d80af16be",
   "metadata": {},
   "source": [
    "### Data Prep & Cleaning Checklist\n",
    "\n",
    "1. Filtered data to only include postseason games from 1999-2025\n",
    "2. Selected features that only pertain to environment, game situation, and context.  Threw out any columns that deal with play result\n",
    "3. Tranformed data to appropriate data type for modeling\n",
    "4. Removed all null values from dataset\n",
    "5. Feature engineering may be necessary, but will run baseline modeling before tackling that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05d114-26b1-4bbc-9370-2642f5aa8757",
   "metadata": {},
   "source": [
    "## XGBOOST MODELING\n",
    "\n",
    "Since the goal of this project is to determine feature importance regarding NFL playoff success, using modeling techniques like Random Forests or eXtreme Gradient Boosting are ideal.  Both techniques will help explain what features were most important in making each decision along the tree.\n",
    "\n",
    "---explain why XGBoost over Random Forests---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7eb9f6-25ea-4ad1-b9f1-670975101665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3e900-735b-4f35-a1de-25e55936857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and target\n",
    "X = model_df.drop(columns=['success', 'game_id'])\n",
    "y = model_df['success']\n",
    "\n",
    "# split data (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize XGB model\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators = 100, \n",
    "    max_depth = 5,\n",
    "    learning_rate = 0.1,\n",
    "    random_state = 42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict and score\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcdbb2-9e4e-46f7-9866-a78b6a1db79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine feature importance \n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='blue')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca421915-1861-4339-8097-12c670ebe637",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run SHAP summary plot\n",
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0dd00-46fd-4337-9620-75ae2f60f908",
   "metadata": {},
   "source": [
    "### BASELINE MODEL OBSERVATIONS\n",
    "\n",
    "1. The model scored with an accuracy ~58% which is solid for a baseline model with barebones features.\n",
    "2. The model values passing plays more than running plays in regards to play success.  This is most likely due to passing plays typically resulting in more net yardage than running plays in the aggregate.  Though, this misses crucial game context.  An overall successful run allows an offense to make the defense focus more on the running game, which opens up more opportunities for successful passing plays, it allows for more time of possession for the offense, it also allows an offense leading late in the game to chew more time off the clock without giving the trailing team opportunities to mount a comeback.  Engineering features to provide the model with this context is key.  Proposed features to add this training context:\n",
    "   - cummalative_run_yards: total rushing yards in the game so far.  The model can see if the defense is being worn down by the run and how it may begin to cheat up with its backers to stop it\n",
    "   - time_of_possession_diff: is there a significant disparity in time of possession and is success in the running game contributing to that\n",
    "   - total_score: to give the model important context on the game score and provide context on team motivation\n",
    "   - posteam_type: provides 'home' and 'away' data to see if the model can determine a homefield advantage, which is typically weighted heavily in game predictions\n",
    "\n",
    "\n",
    "- The field surface does not seem to have any relevant correlation to success or failure\n",
    "- stadium type (indoor vs outdoor) does not seem to have significant affect on success\n",
    "- colder temp does seem to affect play success in that the colder the environment, the greater the chance of play failure\n",
    "- game time remaining has great importance in game result.  More time remaining leads to high chances of a successful result.\n",
    "- The most impactful feature is ydstogo.  The lower the yards to go, the greater chance of successful result.  This is common sense, but proves the model is making realistic predictions based on its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b2f00-b800-4e52-b7c6-033cbea4e997",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING\n",
    "\n",
    "Expanding on the baseline model, more game context needs to be added to achieve a higher accuracy score.\n",
    "\n",
    "The following features will be created and added to the model data:\n",
    "\n",
    "1. total_score\n",
    "2. cumm_run_yards\n",
    "3. top_diff\n",
    "4. possesion_seconds\n",
    "5. posteam_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315d573-f419-41a5-bb5c-fab25d2f2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_score\n",
    "model_df['total_score'] = playoff_master['total_home_score'] + playoff_master['total_away_score']\n",
    "\n",
    "# cumm_run_yards\n",
    "# aggregate 'yards_gained' and filter by 'play_type' == 'rush' from playoff_master\n",
    "playoff_master['temp_rush_yards'] = playoff_master.apply(lambda x: x['yards_gained'] if x['play_type'] == 'run' else 0, axis=1)\n",
    "# group by game and team, sum the yards, the shift(1)\n",
    "model_df['cumm_run_yards'] = playoff_master.groupby(['game_id', 'posteam'])['temp_rush_yards'].transform(\n",
    "    lambda x: x.cumsum().shift(1).fillna(0))\n",
    "\n",
    "# top_diff (time of possession difference) and possession_seconds\n",
    "def time_to_seconds(time_str):\n",
    "    if pd.isna(time_str) or time_str == \"\": return 0\n",
    "    m, s = map(int, time_str.split(':'))\n",
    "    return m * 60 + s\n",
    "\n",
    "model_df['possession_seconds'] = playoff_master['drive_time_of_possession'].apply(time_to_seconds)\n",
    "model_df['top_diff'] = model_df.groupby('game_id')['possession_seconds'].diff().fillna(0)\n",
    "\n",
    "model_df['posteam_type'] = playoff_master['posteam_type']\n",
    "\n",
    "# clean feature set (remove field type identifiers e.g. dome, astroturf, etc as they were shown to be irrelevant)\n",
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', \n",
    "    'score_differential', 'total_score', 'posteam_type', 'is_play_action',\n",
    "    'cumm_run_yards', 'top_diff', 'possession_seconds', 'temp', 'wind'\n",
    "]\n",
    "\n",
    "model_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1154bdc6-9628-4318-92ec-a90a79bdc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.drop(columns=['roof_dome', 'roof_outdoors', 'surface_astroturf',\n",
    "       'surface_dessograss', 'surface_fieldturf', 'surface_grass',\n",
    "       'surface_matrixturf', 'surface_sportturf'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6cddc-165f-4dae-90d7-2abb0adebc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform 'posteam_type' to binary column 'posteam_home' where 1=home team, 0=away team\n",
    "model_df['posteam_home'] = model_df.apply(lambda x: 1 if x['posteam_type'] == 'home' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de5914-178a-4f92-a5c3-9d70164f654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42a252-91b4-4a8c-ad78-d177f0f3edac",
   "metadata": {},
   "source": [
    "## XGBOOST MODEL RUN # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d572d6e-9838-4304-89b6-2c8e6a0bb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', 'score_differential', 'total_score',\n",
    "    'posteam_home', 'cumm_run_yards', 'top_diff', 'possession_seconds', 'temp', 'wind', 'play_type_run'\n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_2 = xgb.XGBClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 0.1,\n",
    "    random_state = 42,\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_2.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb83afd-17ee-49bb-9016-d8a7cad1334f",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING ROUND 2: BUILD FEATURES TO CAPTURE PLAYER TALENT\n",
    "\n",
    "In order to build features that give the model appropriate context on player talent without injecting unnecessary bias into its training, we need a metric that not only properly accesses player value to the team's performance but is from an appropriate historical timeframe.  For this we will use the the average EPA of the player from the regular season associated with the postseason the game occurred in.  For example, we would use Patrick Mahomes average passer EPA from 2023 for the Chiefs 2023 playoff games.  In doing this, the model is only given the value metric for that player for the preceding regular season and is not biased by future or further past performance.  It is only given the most relevant/recent context.\n",
    "\n",
    "1. avg_qb_epa: average passing epa of the quarterback for the preceding regular season\n",
    "2. avg_rush_epa: average rush epa of the runner for the preceding regular season\n",
    "3. avg_target_epa: average target epa of the top 3 receivers for the team for the preceding regular season\n",
    "4. avg_def_epa: average defensive epa of the team defense for the preceding regular season\n",
    "\n",
    "Each metric is also tied to the player_id of the player in question.\n",
    "\n",
    "A **feature injection** approach will be used to map the appropriate epa value to the corresponding play.  In order for the individual player value to be given for each play, the player's average epa value must be given to the model for each play that player is involved in.  However, simply adding the player_ids to the data will not work because then the model will train on that data (which is irrelevant to the goal analysis).  To avoid directly including this player identifying data, we will load the player_id data from playoff_master into a temporary dataframe, then map the specific player epa data to each player_id from playoff_master.  Then create a function to pull the necessary data for each play which results in the correct epa values being added to model_df for each play.  Then the model will be provided the appropriate player skill context that contributed the play's outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf54ad-4973-4bf8-82e3-ec783ddef18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary dataframe that holds the plays in model_df and the player IDs from playoff_master needed for mapping\n",
    "temp_df = model_df.copy()\n",
    "# pull the player IDs from playoff_master\n",
    "temp_df['passer_id'] = playoff_master['passer_player_id']\n",
    "temp_df['rusher_id'] = playoff_master['rusher_player_id']\n",
    "temp_df['receiver_id'] = playoff_master['receiver_player_id']\n",
    "temp_df['season'] = playoff_master['season']\n",
    "temp_df['defteam'] = playoff_master['defteam']\n",
    "\n",
    "## create the talent dictionaries (holds the player epa values)\n",
    "reg_df = df[df['season_type'] == 'REG'].copy()\n",
    "\n",
    "# regular season mean EPA QB\n",
    "qb_talent = reg_df.groupby(['season', 'passer_player_id'])['qb_epa'].mean().to_dict()\n",
    "# regular season mean EPA Rusher\n",
    "rush_talent = reg_df[reg_df['play_type'] == 'run'].groupby(['season', 'rusher_player_id'])['epa'].mean().to_dict()\n",
    "# regular season mean EPA receiver\n",
    "rec_talent = reg_df[reg_df['play_type'] == 'pass'].groupby(['season', 'receiver_player_id'])['epa'].mean().to_dict()\n",
    "# regular season mean team defense epa\n",
    "def_talent = reg_df.groupby(['season', 'defteam'])['epa'].mean().to_dict()\n",
    "\n",
    "## Map the talent (epa values) to the plays\n",
    "\n",
    "# create a helper key (function) for the dict lookup\n",
    "def get_talent(row, id_col, talent_dict):\n",
    "    key = (row['season'], row[id_col])\n",
    "    return talent_dict.get(key, 0)\n",
    "\n",
    "temp_df['avg_qb_epa'] = temp_df.apply(lambda x: get_talent(x, 'passer_id', qb_talent), axis=1)\n",
    "temp_df['avg_rush_epa'] = temp_df.apply(lambda x: get_talent(x, 'rusher_id', rush_talent), axis=1)\n",
    "temp_df['avg_rec_epa'] = temp_df.apply(lambda x: get_talent(x, 'receiver_id', rec_talent), axis=1)\n",
    "temp_df['avg_def_epa'] = temp_df.apply(lambda x: def_talent.get((x['season'], x['defteam']), 0), axis=1)\n",
    "\n",
    "# move epa play values to model_df with no player IDs\n",
    "talent_cols = ['avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'avg_def_epa']\n",
    "model_df[talent_cols] = temp_df[talent_cols]\n",
    "\n",
    "print(model_df[talent_cols].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631dba9c-d3d0-4bf7-a790-e8cbade0967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['mean_qb_epa', 'mean_rush_epa', 'mean_target_epa', 'mean_def_epa', 'avg_target_epa']\n",
    "model_df = model_df.drop(columns=cols, errors='ignore')\n",
    "\n",
    "model_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf4335-6f15-420c-81f6-d347fcd9287c",
   "metadata": {},
   "source": [
    "## XGBOOST MODEL RUN #3: PLAYER TALENT FEATURES ADDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d067c3-7680-4a87-9387-e326d417e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', 'score_differential', 'total_score',\n",
    "    'posteam_home', 'cumm_run_yards', 'top_diff', 'possession_seconds', 'temp', 'wind', 'play_type_run',\n",
    "    'avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'avg_def_epa'\n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_3 = xgb.XGBClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 0.1,\n",
    "    random_state = 42,\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "\n",
    "model_3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_3.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c697b9-7e9d-48c3-b8e8-102a76504b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attempt RandomForest modeling, less sensitive to noisy player data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', 'score_differential', 'total_score',\n",
    "    'posteam_home', 'cumm_run_yards', 'top_diff', 'possession_seconds', 'temp', 'wind', 'play_type_run',\n",
    "    'avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'avg_def_epa']\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "print(f'Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae39c3-72a2-47fe-a03a-909475f1a63c",
   "metadata": {},
   "source": [
    "The model appears to still have issues identifying successful plays.  It is leaning to much on the majority class (failure).  In order to nudge it a bit we will attempt to strategies:\n",
    "\n",
    "1. define situational matchup featuresL qb_matchup_epa, rush_matchup_epa which will calculate whether the offense or defense has a 'edge'\n",
    "2. tweak scale_pos_weight parameter in XGBoost to 1.3 to force the model to learn more about successful plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8880cca-a573-4399-8c41-7c1352a57233",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first test the second strategy: tweak the scale_pos_weight parameter\n",
    "\n",
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', 'score_differential', 'total_score',\n",
    "    'posteam_home', 'cumm_run_yards', 'top_diff', 'possession_seconds', 'temp', 'wind', 'play_type_run',\n",
    "    'avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'avg_def_epa'\n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_4 = xgb.XGBClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state = 42,\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "\n",
    "model_4.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_4.predict(X_test)\n",
    "print(f'Model 4 Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print('\\nModel 4 Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4cb9d5-ca5e-45e0-b048-8e1d3ebc09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add in matchup epa features\n",
    "model_df['qb_matchup_epa'] = model_df['avg_qb_epa'] - model_df['avg_def_epa']\n",
    "model_df['rush_matchup_epa'] = model_df['avg_rush_epa'] - model_df['avg_def_epa']\n",
    "\n",
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', 'score_differential', 'total_score',\n",
    "    'posteam_home', 'cumm_run_yards', 'top_diff', 'possession_seconds', 'temp', 'wind', 'play_type_run',\n",
    "    'avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'avg_def_epa', 'qb_matchup_epa', 'rush_matchup_epa'\n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_4a = xgb.XGBClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state = 42,\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "\n",
    "model_4a.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_4a.predict(X_test)\n",
    "print(f'Model 4a Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print('\\nModel 4a Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95e5b7-e120-45ed-9879-a9c98b84d768",
   "metadata": {},
   "source": [
    "### OBSERVATIONS ON MODELING SO FAR AND THE < 70% ACCURACY WALL\n",
    "\n",
    "1. It was assumed that engineering in more robust situational features would increase model accuracy, which it did (58% -> 68%)\n",
    "2. It was assumed that adding in specific player talent context (the mean EPA metrics) would greatly increase model accuracy, but it seems to have hit a wall at 68%.\n",
    "3. Adding weighting to the parameters did increase recall and allow the model to find more successful plays, however adding matuchup metrics was revealed to be irrelevant to the model.\n",
    "\n",
    "Ideas to Investigate Reasons for the Accuracy Wall:\n",
    "\n",
    "1. Look into how many plays actually have talent data\n",
    "2. add yardline verticality context (red zone vs own 20 yard line)\n",
    "3. add cummulative success metric that provides drive momentum score (e.g. last_3_plays_epa, plays_this_drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d728a6-5924-4dcd-8d69-6d41b216debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('% of plays with talent data:')\n",
    "print((model_df[['avg_qb_epa', 'avg_rush_epa']] != 0).mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacb8ca-4136-4202-b8c3-f8a645a25355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AVG EPA Columns with Meaningful Data')\n",
    "for col in ['avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'avg_def_epa']:\n",
    "    real_data_count = (model_df[col] != 0).sum()\n",
    "    zero_count = (model_df[col] == 0).sum()\n",
    "    print(f'{col}: {real_data_count} plays have data, {zero_count} plays are 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a30903-1f43-405a-ad4b-2e0d28b8cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = playoff_master['passer_player_id'].dropna().iloc if not playoff_master['passer_player_id'].dropna().empty else \"None Found\"\n",
    "print(f'Sample ID from Playoff_Master: {sample_id}')\n",
    "\n",
    "reg_ids = df[df['season_type'] == 'REG']['passer_player_id'].unique()\n",
    "is_match = sample_id in reg_ids\n",
    "print(is_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8b75e-435d-42b1-98af-ba31fafedea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Team-Level Scouting Reports from Regular Season\n",
    "reg_df = df[df['season_type'] == 'REG'].copy()\n",
    "\n",
    "# Averages by Season and Team\n",
    "qb_map = reg_df.groupby(['season', 'posteam'])['qb_epa'].mean().to_dict()\n",
    "rush_map = reg_df[reg_df['play_type'] == 'run'].groupby(['season', 'posteam'])['epa'].mean().to_dict()\n",
    "def_map = reg_df.groupby(['season', 'defteam'])['epa'].mean().to_dict()\n",
    "\n",
    "# Create mapping keys in playoff_master\n",
    "playoff_master['team_key'] = list(zip(playoff_master['season'], playoff_master['posteam']))\n",
    "playoff_master['def_key'] = list(zip(playoff_master['season'], playoff_master['defteam']))\n",
    "\n",
    "#Map the data to model_df\n",
    "model_df['avg_qb_epa'] = playoff_master['team_key'].map(qb_map).fillna(0)\n",
    "model_df['avg_rush_epa'] = playoff_master['team_key'].map(rush_map).fillna(0)\n",
    "model_df['avg_def_epa'] = playoff_master['def_key'].map(def_map).fillna(0)\n",
    "\n",
    "# Create the Matchup Features \n",
    "model_df['qb_matchup_epa'] = model_df['avg_qb_epa'] - model_df['avg_def_epa']\n",
    "model_df['rush_matchup_epa'] = model_df['avg_rush_epa'] - model_df['avg_def_epa']\n",
    "\n",
    "# verify amount of rows with data\n",
    "print((model_df[['avg_qb_epa', 'avg_rush_epa', 'avg_def_epa']] != 0).mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a314d-80f7-4364-84a0-b8fd0f36813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', 'score_differential', 'total_score',\n",
    "    'posteam_home', 'cumm_run_yards', 'top_diff', 'possession_seconds', 'temp', 'wind', 'play_type_run',\n",
    "    'avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'avg_def_epa', 'qb_matchup_epa', 'rush_matchup_epa'\n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_5 = xgb.XGBClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state = 42,\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "\n",
    "model_5.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_5.predict(X_test)\n",
    "print(f'Model 5 Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print('\\nModel 5 Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a09bbd-6d04-4513-a17c-865df9ce382a",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "\n",
    "After fixing the EPA mapping issues and populating over 99% of the rows with player epa data, the model remains stuck at < 70% accuracy and none of the other scores moved significantly.  This essentially confirms that the talent features (epa metrics) are not relevant at all to the model in its training.  These factors must already be baked into game situation data like score differential, etc.  \n",
    "\n",
    "We will attempt feature importance plotting to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70e491-dc47-4e92-bf33-6563c81c5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importance = model_5.get_booster().get_score(importance_type='gain')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh([x[0] for x in importance], [x[1] for x in importance])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f158e-5ddb-4da2-a8bb-f9a084331d34",
   "metadata": {},
   "source": [
    "## OBSERVATIONS\n",
    "\n",
    "- **Dominance of Passing Talent**: 'avg_rec_epa' is the single most important feature in the model.  The confirms that the model highly values an elite passing game, and more directly an elite receiving corps as the primary driver of play success.\n",
    "- **Context Still Key**:  'play_type_run' and 'possession_seconds' remain in the top 3 of feature importance.  This confirms that while talent is of importance, the specific strategic context and game control are still critical foundational elements that drive play success\n",
    "- **Rushing vs Passing**:  'avg_rec_epa' is valued far higher than 'avg_rush_epa' by the model.  This intuitively makes sense since passing plays typically result in higher yardage gains than runs.  However, the current structure of the features is not providing the model with enough context about an effective run game in the broader context of gameplay.  A strong run game allows a team to control possession more, forces the defense to cheat for the run, opening up the passing game, and also tires the defense out later in the game.  Features will have to be added to provide this context\n",
    "- **QB vs Target Value**:  The model currently values the production of the receivers highly over the production over the QB.  This goees against common NFL knowledge where the talent of the QB is viewed as far more important to team success than the talent of the receivers.  This warrants investigation\n",
    "- **Feature Engineering Ideas**:  add features that provide context on QB advantage and rushing advantage versus the defense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198e535-4fb9-4fc2-a5db-9500194af154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply avg_qb_epa by 1 - whatever value is in play_type_run (1 or 0) which will end up\n",
    "# avg_qb_epa * 1 on pass plays and avg_qb_epa * 0 on run plays\n",
    "model_df['active_qb_epa'] = model_df['avg_qb_epa'] * (1 - model_df['play_type_run'])\n",
    "model_df['active_rec_epa'] = model_df['avg_rec_epa'] * (1 - model_df['play_type_run'])\n",
    "\n",
    "# perform a straight multiplication of avg_rush_epa * play_type_run\n",
    "# results in 0 when its a pass play and avg_rush_epa when a run\n",
    "model_df['active_rush_epa'] = model_df['avg_rush_epa'] * model_df['play_type_run']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759870af-3859-44a4-a438-75542c3c751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', 'score_differential', 'total_score',\n",
    "    'posteam_home', 'cumm_run_yards', 'top_diff', 'possession_seconds', 'temp', 'wind', 'play_type_run',\n",
    "    'avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'avg_def_epa', 'qb_matchup_epa', 'rush_matchup_epa'\n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_6 = xgb.XGBClassifier(\n",
    "    n_estimators = 100,\n",
    "    max_depth = 5,\n",
    "    learning_rate = 0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state = 42,\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "\n",
    "model_6.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_6.predict(X_test)\n",
    "print(f'Model 6 Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print('\\nModel 6 Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e8525-0536-4432-883f-51649c4990c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add efficiency metrics\n",
    "\n",
    "model_df['yards_to_goal_diff'] = model_df['ydstogo'] / model_df['yardline_100']\n",
    "\n",
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', \n",
    "    'score_differential', 'total_score', 'posteam_home',\n",
    "    'cumm_run_yards', 'top_diff', 'possession_seconds', \n",
    "    'temp', 'wind', 'play_type_run',\n",
    "    'active_qb_epa', 'active_rush_epa', 'active_rec_epa', \n",
    "    'avg_def_epa', 'qb_matchup_epa', 'rush_matchup_epa',\n",
    "    'yards_to_goal_diff' \n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_7 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model_7.fit(X_train, y_train)\n",
    "y_pred = model_7.predict(X_test)\n",
    "\n",
    "print(f\"Model 7 Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06492e85-08e2-410d-b77f-10c435e5ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add scoring scoring context metrics\n",
    "model_df['is_redzone'] = (model_df['yardline_100'] <= 20).astype(int)\n",
    "model_df['is_goal_to_go'] = (model_df['ydstogo'] == model_df['yardline_100']).astype(int)\n",
    "model_df['is_fg_range'] = (model_df['yardline_100'] <=40).astype(int)\n",
    "\n",
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', \n",
    "    'score_differential', 'total_score', 'posteam_home',\n",
    "    'cumm_run_yards', 'top_diff', 'possession_seconds', \n",
    "    'temp', 'wind', 'play_type_run',\n",
    "    'active_qb_epa', 'active_rush_epa', 'active_rec_epa', \n",
    "    'avg_def_epa', 'qb_matchup_epa', 'rush_matchup_epa',\n",
    "    'yards_to_goal_diff', 'is_redzone', 'is_goal_to_go', 'is_fg_range'\n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_8 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model_8.fit(X_train, y_train)\n",
    "y_pred = model_8.predict(X_test)\n",
    "\n",
    "print(f\"Model 8 Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146128f-1a86-42c7-a493-ac65b7235072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importance = model_8.get_booster().get_score(importance_type='gain')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh([x[0] for x in importance], [x[1] for x in importance])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e21dd-873d-4900-afed-defca744a649",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "- **Irrelevant Context Metrics:** The new scoring context metrics (is_redzone, is_goal_to_go, is_fg_range) appear to be irrelevant to the model and do not appear in the Feature Importance plot. This is likely due to the spike in yards_to_goal_diff importance; the model derives sufficient scoring potential information from that single metric. We will drop these three flags from the feature set.\n",
    "\n",
    "- **Receiver vs. QB Value:** A surprising finding is that the model values active_rec_epa (receiver production) significantly higher than active_qb_epa (QB production). This contradicts standard NFL wisdom where the Quarterback is viewed as the primary driver of success. This suggests the model might be over-indexing on the result of passing plays (receivers making plays) rather than the process (QB play), or it implies that elite receiving corps are a stronger differentiator in this specific dataset.\n",
    "\n",
    "- **Undervalued Run Game:** avg_rush_epa remains low in importance compared to passing metrics. While passing generally yields more EPA, the current feature set fails to capture the broader strategic value of a strong run game (possession control, wearing down defenses, setting up play-action).\n",
    "\n",
    "- **Breaking the Accuracy Wall:** To improve upon the ~68% accuracy plateau and address these context gaps, we will add the following features:\n",
    "\n",
    "- yards_gained_prev_play: To give the model a sense of immediate momentum and game flow.\n",
    "\n",
    "- clutch_index: To differentiate high-pressure 4th quarter decisions from low-leverage 1st quarter plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675dce1d-73dd-42a2-a0fe-7e68641a6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['yards_gained_prev_play'] = df.groupby('game_id')['yards_gained'].shift(1).fillna(0)\n",
    "model_df['clutch_index'] = abs(model_df['score_differential']) / (model_df['game_seconds_remaining'] + 1)\n",
    "\n",
    "features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining', \n",
    "    'score_differential', 'total_score', 'posteam_home',\n",
    "    'cumm_run_yards', 'top_diff', 'possession_seconds', \n",
    "    'temp', 'wind', 'play_type_run',\n",
    "    'active_qb_epa', 'active_rush_epa', 'active_rec_epa', \n",
    "    'avg_def_epa', 'qb_matchup_epa', 'rush_matchup_epa',\n",
    "    'yards_to_goal_diff', 'yards_gained_prev_play', 'clutch_index'\n",
    "]\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model_9 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model_9.fit(X_train, y_train)\n",
    "y_pred = model_9.predict(X_test)\n",
    "\n",
    "print(f\"Model 9 Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "importance = model_9.get_booster().get_score(importance_type='gain')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh([x[0] for x in importance], [x[1] for x in importance])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a2da2-60b0-48b5-a181-63f6eb0916b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "participation_df = nfl.load_participation(seasons=range(2016, 2025)).to_pandas()\n",
    "if 'nflverse_game_id' in participation_df.columns:\n",
    "    participation_df = participation_df.rename(columns={'nflverse_game_id': 'game_id'})\n",
    "\n",
    "participation_df['play_id'] = participation_df['play_id'].astype(int)\n",
    "\n",
    "model_df_modern = model_df.merge(\n",
    "    participation_df[['game_id', 'play_id', 'offense_formation', 'offense_personnel', 'defenders_in_box', 'number_of_pass_rushers']],\n",
    "    on=['game_id', 'play_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "model_df_modern['offense_formation'] = model_df_modern['offense_formation'].fillna('Unknown')\n",
    "model_df_modern['offense_personnel'] = model_df_modern['offense_personnel'].fillna('Unknown')\n",
    "\n",
    "model_df_modern['num_rb'] = model_df_modern['offense_personnel'].str.extract(r'(\\d)\\s*RB').fillna(-1).astype(int)\n",
    "model_df_modern['num_te'] = model_df_modern['offense_personnel'].str.extract(r'(\\d)\\s*TE').fillna(-1).astype(int)\n",
    "\n",
    "model_df_modern['personnel_group'] = (model_df_modern['num_rb'] * 10) + model_df_modern['num_te']\n",
    "\n",
    "target_groups = [11, 12, 21, 10, 13, 22, 2]\n",
    "model_df_modern['personnel_clean'] = model_df_modern['personnel_group'].where(model_df_modern['personnel_group'].isin(target_groups), 'Other').astype(str)\n",
    "\n",
    "model_df_modern['defenders_in_box'] = model_df_modern['defenders_in_box'].fillna(-1)\n",
    "model_df_modern['number_of_pass_rushers'] = model_df_modern['number_of_pass_rushers'].fillna(-1)\n",
    "\n",
    "form_dummies = pd.get_dummies(model_df_modern['offense_formation'], prefix='form')\n",
    "pers_dummies = pd.get_dummies(model_df_modern['personnel_clean'], prefix='pers')\n",
    "\n",
    "model_df_modern = pd.concat([model_df_modern.reset_index(drop=True), form_dummies.reset_index(drop=True), pers_dummies.reset_index(drop=True)], axis=1)\n",
    "\n",
    "new_dummy_cols = [col for col in model_df_modern.columns if col.startswith('form_') or col.startswith('pers_')]\n",
    "base_features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining',\n",
    "    'score_differential', 'posteam_timeouts_remaining', 'defteam_timeouts_remaining',\n",
    "    'temp', 'wind', 'shotgun', 'no_huddle',\n",
    "    'defenders_in_box', 'number_of_pass_rushers', 'active_qb_epa', 'active_rush_epa', 'active_rec_epa', \n",
    "    'avg_def_epa', 'qb_matchup_epa', 'rush_matchup_epa',\n",
    "    'yards_to_goal_diff', 'yards_gained_prev_play', 'clutch_index'\n",
    "]\n",
    "modern_features = base_features + new_dummy_cols\n",
    "\n",
    "model_df_modern = model_df_modern.sort_values(by=['game_id', 'play_id']).reset_index(drop=True)\n",
    "\n",
    "X = model_df_modern[modern_features]\n",
    "y = model_df_modern['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_10 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "model_10.fit(X_train, y_train)\n",
    "y_pred = model_10.predict(X_test)\n",
    "\n",
    "print(f\"Model 10 Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e66d7-2026-4190-b3ec-ff0226856e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model_10.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_n = 20\n",
    "top_features = sorted_importance[:top_n]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh([x[0] for x in top_features], [x[1] for x in top_features], color='blue')\n",
    "plt.xlabel('Gain')\n",
    "plt.title('Top 20 Feature Importance - Model 10')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e9330-6026-4308-8674-6ef02967d9c5",
   "metadata": {},
   "source": [
    "### OBSERVATIONS ON MODEL 10\n",
    "\n",
    "- the model continues to highly value active_rec_epa.  I believe this may be because of data leakage.  Plays that are not a pass do not include a active_rec_epa and those that are a pass, do.  Therefore, the model sees a high active_rec_epa on a pass play and is easily able to determine the outcome of the play without viewing the other features.  Essentially, it is cheating on coming to the outcome of successful plays.  Having this feature in the feature set is allowing the model to cheat.  Dropping active_rec_epa, active_rush_epa, and rush_matchup_epa (which all allow the model to guess the play outcome without accessing the other features) should make the feature importance more realistic.  This will also likely decrease model accuracy greatly, but it will provide a more realistic baseline to build upon.\n",
    "- focus more on adding situational and game context features that the model cannot cheat with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e536795-4d19-4110-8ff7-38cbf8cbfd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining',\n",
    "    'score_differential', 'posteam_timeouts_remaining', 'defteam_timeouts_remaining',\n",
    "    'temp', 'wind', 'shotgun', 'no_huddle',\n",
    "    'defenders_in_box', 'number_of_pass_rushers', 'active_qb_epa', \n",
    "    'avg_def_epa', 'qb_matchup_epa',\n",
    "    'yards_to_goal_diff', 'yards_gained_prev_play', 'clutch_index'\n",
    "]\n",
    "modern_features = base_features + new_dummy_cols\n",
    "\n",
    "model_df_modern = model_df_modern.sort_values(by=['game_id', 'play_id']).reset_index(drop=True)\n",
    "\n",
    "X = model_df_modern[modern_features]\n",
    "y = model_df_modern['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_11 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "model_11.fit(X_train, y_train)\n",
    "y_pred = model_11.predict(X_test)\n",
    "\n",
    "print(f\"Model 11 Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c4b23-aed4-45b2-b31e-82b3fb393504",
   "metadata": {},
   "source": [
    "### OBSERVATIONS ON MODEL 11\n",
    "\n",
    "- as expected, after removing the rushing and receiving metrics, the model's accuracy and precision dropped noticeably.  It was no longer able to hyperfocus on those features, leading to successful predictions.  However, the having those features damaged the validity of those predictions.  This is a better foundation to build future models off of\n",
    "- future features need to focus more on pre-snap data that does not signal what the following play result will be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb2135-8340-4f13-a106-12867a667c64",
   "metadata": {},
   "source": [
    "### FIXES NEEDED FOR EPA METRICS: POSSIBLE DATA LEAKAGE CORRECTION\n",
    "\n",
    "Transforming the EPA Metrics in the Feature Set\n",
    "-----------------------------------------------\n",
    "\n",
    "1. Remove active_rec_epa completely as it leaks data about the play result and allows the model to cheat\n",
    "2. Transform the active_rush_epa so it gives the model context about the overall team rush skill, rather than specific player rush skill.  This allows the model to have context about how well the team performs overall at rushing, without leaking data like \"this specific player is getting the ball on this play and running, so it is not a pass play\"\n",
    "3. Remove the scouting report section of the code as this degrades the qb_epa data.  The model needs to know how a specific qb has performed in the regular season for said team.  The 'scouting report' logic aggregates the qb data into team-wide data which can be misleading is the starting qb is not available on that specific play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c88ff7b-97bc-4f2f-8c0e-a11a6f89d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring id columns into model_df_modern (merge from playoff_master\n",
    "cols_to_add = ['game_id', 'play_id', 'season', 'posteam', 'defteam', 'passer_player_id']\n",
    "temp_ids = playoff_master[cols_to_add].copy()\n",
    "# drop redundant/old data from model_df_modern\n",
    "modern_df_modern = model_df_modern.drop(columns=[c for c in ['season', 'posteam', 'defteam', 'passer_player_id'] if c in model_df_modern.columns], errors='ignore')\n",
    "# merge ids back into model_df_modern\n",
    "model_df_modern = model_df_modern.merge(\n",
    "    temp_ids, \n",
    "    on=['game_id', 'play_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# prepare the source data (only regular season data)\n",
    "reg_df = df[df['season_type'] == 'REG'].copy()\n",
    "\n",
    "# build dictionaries \n",
    "# QB: player specific data and only the preceding regular season\n",
    "qb_talent = reg_df.groupby(['season', 'passer_player_id'])['qb_epa'].mean().to_dict()\n",
    "\n",
    "# Rush, Defense, QB (for fallback for NaN values : team level and only the preceding regular season\n",
    "rush_map = reg_df[reg_df['play_type'] == 'run'].groupby(['season', 'posteam'])['epa'].mean().to_dict()\n",
    "def_map = reg_df.groupby(['season', 'defteam'])['epa'].mean().to_dict()\n",
    "team_qb_map = reg_df.groupby(['season', 'posteam'])['qb_epa'].mean().to_dict()\n",
    "\n",
    "# map new features\n",
    "# create mapping keys\n",
    "model_df_modern['team_key'] = list(zip(model_df_modern['season'], model_df_modern['posteam']))\n",
    "model_df_modern['def_key'] = list(zip(model_df_modern['season'], model_df_modern['defteam']))\n",
    "\n",
    "model_df_modern['active_rush_epa'] = model_df_modern['team_key'].map(rush_map).fillna(0)\n",
    "model_df_modern['active_def_epa'] = model_df_modern['def_key'].map(def_map).fillna(0)\n",
    "\n",
    "\n",
    "# active qb EPA (player specific)\n",
    "model_df_modern['qb_player_key'] = list(zip(model_df_modern['season'], model_df_modern['passer_player_id']))\n",
    "model_df_modern['active_qb_epa'] = model_df_modern['qb_player_key'].map(qb_talent)\n",
    "# fill NaN (run plays) with team average\n",
    "model_df_modern['active_qb_epa'] = model_df_modern['active_qb_epa'].fillna(model_df_modern['team_key'].map(team_qb_map))\n",
    "# handle edge cases\n",
    "model_df_modern['active_qb_epa'] = model_df_modern['active_qb_epa'].fillna(0)\n",
    "\n",
    "# recalculate matchups\n",
    "model_df_modern['qb_matchup_epa'] = model_df_modern['active_qb_epa'] - model_df_modern['active_def_epa']\n",
    "model_df_modern['rush_matchup_epa'] = model_df_modern['active_rush_epa'] - model_df_modern['active_def_epa']\n",
    "\n",
    "# cleanup columns\n",
    "model_df_modern.drop(columns=['team_key', 'def_key', 'qb_player_key'], inplace=True)\n",
    "\n",
    "# Verify changes \n",
    "print(\"Data Leak check: 'avg_rec_epa' is present?\" 'avg_rec_epa' in model_df_modern.columns)\n",
    "print('\\nNew Feature Sample:')\n",
    "print(model_df_modern[['active_qb_epa', 'active_rush_epa', 'active_def_epa', 'qb_matchup_epa', 'rush_matchup_epa']].head())\n",
    "print(model_df_modern[['active_qb_epa', 'active_rush_epa', 'active_def_epa', 'qb_matchup_epa', 'rush_matchup_epa']].info())\n",
    "print(model_df_modern[['active_qb_epa', 'active_rush_epa', 'active_def_epa', 'qb_matchup_epa', 'rush_matchup_epa']].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30804f-89e1-4aac-b734-637b0fed1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 12 with feature corrections\n",
    "\n",
    "# define feature set\n",
    "base_features = [\n",
    "    'down', 'ydstogo', 'yardline_100', 'game_seconds_remaining',\n",
    "    'score_differential', 'posteam_timeouts_remaining', 'defteam_timeouts_remaining',\n",
    "    'temp', 'wind', 'shotgun', 'no_huddle',\n",
    "    'defenders_in_box', 'number_of_pass_rushers', 'active_qb_epa', \n",
    "    'avg_def_epa', 'qb_matchup_epa',\n",
    "    'yards_to_goal_diff', 'yards_gained_prev_play', 'clutch_index'\n",
    "] \n",
    "\n",
    "# add formation and personnel dummy columns\n",
    "dummy_cols=[col for col in model_df_modern.columns if col.startswith('form_') or col.startswith('pers_')]\n",
    "modern_features = base_features + dummy_cols\n",
    "\n",
    "# sort for reproducability\n",
    "model_df_modern = model_df_modern.sort_values(by=['game_id', 'play_id']).reset_index(drop=True)\n",
    "\n",
    "X = model_df_modern[modern_features]\n",
    "y = model_df_modern['success']\n",
    "\n",
    "# split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_12 = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1.3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model_12.fit(X_train, y_train)\n",
    "y_pred = model_12.predict(X_test)\n",
    "\n",
    "print(f'Model 12 Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# plot feature importance \n",
    "importance = model_12.get_booster().get_score(importance_type='gain')\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "top_n = 20\n",
    "top_features = sorted_importance[:top_n]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh([x[0] for x in top_features], [x[1] for x in top_features], color='blue')\n",
    "plt.xlabel('Gain')\n",
    "plt.title('Model 12 Feature Importance: Top 20')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ceeed-0130-4d97-b5e9-05fbe1bdd9e6",
   "metadata": {},
   "source": [
    "### HYPERPARAMETER TESTING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66626123-252b-493b-9636-c9d38e4a5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# define grid of parameters to test\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'scale_pos_weight': [1.2, 1.3, 1.4]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "# setup RandomSearch\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# fit the search\n",
    "print('Starting Hyperparameter Tuning...')\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# return best version of model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(f'Best Parameters Found: {random_search.best_params_}')\n",
    "\n",
    "# evaluate best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(f'\\nAccuracy of best model: {accuracy_score(y_test, y_pred_best):.4f}')\n",
    "print(classification_report(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5be847-e8bc-450a-b254-c5141acd19b6",
   "metadata": {},
   "source": [
    "### FEATURE ISOLATION TESTING\n",
    "\n",
    "- Hyperparameter tuning confirms the issue is not the model parameters.  This is a feature issue.\n",
    "- Is the issue garbage features created excessive noise? Or is the issue a lack of strong, meaningful features for the model to learn on\n",
    "- We will isolate the features and test them for validity/noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3468ca-3513-4493-b008-b3e8978477a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(X_test.columns[perm_sorted_idx][-20:], result.importances_mean[perm_sorted_idx][-20:])\n",
    "plt.title('Permutation Importance')\n",
    "plt.show()\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f37ca-b2b6-469d-a270-060aabf33ea2",
   "metadata": {},
   "source": [
    "### FEATURE IMPORTANCE OBSERVATIONS\n",
    "\n",
    "- Unfortunately, most of the features that we engineered and added are either pointless to the model or creating too much noise for it to learn anything relevant.\n",
    "- It is clear that dropping most of the feature set is necessary, keeping only ydstogo, active_qb_epa, down, yards_to_goal_diff, yardline_100 as these are the only features that appear to be of significant relevance to the mdoel's training.\n",
    "- However, these are not significant enough to provide sufficient context for the model to predict per play success.  We need to build more aggregated features that encompass much of the data we were feeding the model previously.  Hopefully aggregating that data will reduce column amount and noise enough for the model to be able to find meaning.\n",
    "\n",
    "Ideas for New Aggregated Features\n",
    "\n",
    "1. box_mismatch: the differential between 'defenders_in_the_box' and 'offense_personnel'.  Essentially, how many players does the offense have in the box vs how many the defense has in the box.\n",
    "2. is_passing_down: currently the model highly values down and ydstogo.  But 3rd & 1 is a completely different situation than 3rd & 12.  They require different offensive strategy for success.  The model needs context to learn this.  Same for early downs vs later downs (1st down vs 3rd down).  This metric will hopefully build that context.\n",
    "3. team_game_success_rate: tells the model what percentage of the team's last 10 plays were a success.  Football is very a sport that relies on team momentum.  As a team 'gets on a roll' or has repeated success, this tends to lend to future success.  We need to build this context into the model's training.\n",
    "\n",
    "- Hyperparameter tuning suggested scale_pos_weight=1.2 as the best value, but precision and recall dropped significantly when using that.  We need to tell that model that missing \"Success\" is far more expensive than misclassifying a \"Failure\" by raising this parameter (1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0aed03-d74a-42bf-871f-f03fea2e9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    df = df.sort_values(by=['game_id', 'play_id'])\n",
    "\n",
    "    # 'box_mismatch': consolidate all personnel cols and 'defenders_in_box' into one differential metric\n",
    "    df['num_blockers'] = 5 + df['num_te'] + df['num_rb']\n",
    "    df['box_mismatch'] = df['num_blockers'] = df['defenders_in_box']\n",
    "\n",
    "    # Handle -1 values from seasons prior to 2016\n",
    "    df.loc[df['defenders_in_box'] == -1, 'box_mismatch'] = 0\n",
    "\n",
    "    # is_passing_down: 3rd or 4th down AND > 4 yards to go, 1st or 2nd down AND > 10 yards to go\n",
    "    df['is_passing_down'] = (((df['down'] >= 3) & (df['ydstogo'] >= 5)) | (df['ydstogo'] >= 10)).astype(int)\n",
    "\n",
    "    # team_game_success_rate: how has the team performed on last 10 plays\n",
    "    df['team_game_success_rate'] = df.groupby('game_id')['success'].transform(\n",
    "        lambda x: x.shift(1).rolling(window=10, min_periods=1).mean()\n",
    "    ).fillna(0.45) # league average baseline\n",
    "\n",
    "    return df\n",
    "\n",
    "# apply the new features to model_df_modern\n",
    "model_df_modern = engineer_features(model_df_modern)\n",
    "\n",
    "# define new feature set based on those that showed up in importance plot or are direct inputs to new metrics\n",
    "new_features = [\n",
    "    'ydstogo', 'down', 'active_qb_epa', 'yards_to_goal_diff', 'yardline_100', 'number_of_pass_rushers', 'avg_def_epa',\n",
    "    'box_mismatch', 'is_passing_down', 'team_game_success_rate', 'active_rush_epa', 'qb_matchup_epa', 'rush_matchup_epa'\n",
    "]\n",
    "\n",
    "X = model_df_modern[new_features]\n",
    "y = model_df_modern['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "model_13 = xgb.XGBClassifier(\n",
    "    subsample=0.7, \n",
    "    scale_pos_weight=1.3, \n",
    "    n_estimators=1500, \n",
    "    max_depth=4, \n",
    "    learning_rate=0.01, \n",
    "    gamma=0.0, \n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# stop early if no improvement in accuracy after 500 rounds\n",
    "model_13.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f'Best Iteration: {model_13.best_iteration}')\n",
    "\n",
    "y_pred = model_13.predict(X_test)\n",
    "\n",
    "print(f'Model 13 Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1500742-5361-4e31-b71a-77589406df97",
   "metadata": {},
   "source": [
    "### OBSERVATIONS ON MODEL 13\n",
    "\n",
    "- using 2016-2025 for the participation data (formations, personnel) and 1999-2025 for the plays and all other data is causing issues.  The model is unable to deal with all the noise created by the 0 values in the participation data.\n",
    "- We will have to drop the pre-2016 data and plays as after many different strategies to keep it and eliminate the noise, its ultimately plateaued the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55ffc4-6177-4c68-b3f3-5c772dd00d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_only_df = model_df_modern[model_df_modern['defenders_in_box'] != -1].copy()\n",
    "\n",
    "print(f'Original dataset size: {len(model_df_modern)}')\n",
    "print(f'2016-2025 Dataset Size: {len(modern_only_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75898e2e-f37b-4582-9d4a-d7aadec5b10c",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "\n",
    "Filtering our dataset from ~39000 rows down to ~13700 rows is a large reduction.  XGBoost is still the appropriate model to run for this project, but the parameters will need to be altered to accomodate the smaller dataset\n",
    "\n",
    "1. max_depth must be decreased from 4 to 3.  This will force the model to learn only the most powerful signals, instead of getting into the weeds with niche cases.\n",
    "2. min_child_weight increased to 7 to make sure the model does not create a new leaf unless it contains at least 7 plays.  Prevents the model from creating rules based on outliers.\n",
    "3. subsample and colsample_bytree need to be set lower around 0.7 to force the model generalize more than relying heavily on dominant features\n",
    "4. gamma: increase to 1.5 to force model to not make a split unless it improves accuracy by a significant margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d94919-ea8a-4c09-a612-89bcfb2432d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_only_df = model_df_modern[model_df_modern['defenders_in_box'] != -1].copy()\n",
    "\n",
    "features = [\n",
    "    'ydstogo', 'down', 'active_qb_epa', 'yards_to_goal_diff', 'yardline_100', 'number_of_pass_rushers', 'avg_def_epa',\n",
    "    'box_mismatch', 'is_passing_down', 'team_game_success_rate', 'active_rush_epa', 'qb_matchup_epa', 'rush_matchup_epa'\n",
    "]\n",
    "\n",
    "X = modern_only_df[features]\n",
    "y = modern_only_df['success']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "modern_model_1 = xgb.XGBClassifier(\n",
    "    max_depth=3,\n",
    "    min_child_weight=7,\n",
    "    gamma=1.5,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    scale_pos_weight=1.3,\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "modern_model_1.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f'Best Iteration: {modern_model_1.best_iteration}')\n",
    "y_pred = modern_model_1.predict(X_test)\n",
    "\n",
    "print(f'Modern Only v1 Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74308622-6f94-48e9-9717-fe27060682b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## experiment with Logistic Regression modeling to see if a simpler model technique produces higher accuracy\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(f'LogReg Accuracy: {accuracy_score(y_test, y_pred_log):4f}')\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "## SHAP Analysis\n",
    "\n",
    "explainer = shap.TreeExplainer(modern_model_1)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('SHAP Summary: Modern Only Model v1')\n",
    "shap.summary_plot(shap_values, X_test, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43732b67-7fc8-4cf1-82e6-542bee7ab150",
   "metadata": {},
   "source": [
    "### SHAP SUMMARY INTERPRETATIONS\n",
    "\n",
    "1. 'ydstogo': when ydstogo is high, the model is correctly predicting likely play failure\n",
    "2. 'number_of_pass_rushers': this appears to be backwards, when the number of pass rushers is low, the chance of play success should increase, not decrease \n",
    "3. 'box_mismatch':  this partially makes sense.  as the differential increases in the offense's favors (when they have more blockers) their success probability decreases.  This makes sense in that when an offense has more blockers on the line, they are typically running, which leads to shorter yardage gains.  Though, it looks like the model may be interpreting this incorrectly.  Either way, it values this feature the least of the ones in the plot.\n",
    "4. active_qb_epa surprisingly does not have the impact it should.  a weaker qb correctly decreases likelihood of play success, but stronger qbs do not seem to move the needle towards succes much.\n",
    "5. team_game_success_rate seems to be interpreted by the model as noise\n",
    "6. 'down': the model seems to be interpreting this correctly.  Higher value in 'down' (3rd or 4th) end in failure more.  lower number in 'down' (1st or 2nd) are slightly more successful and some higher number downs are also successful.\n",
    "\n",
    "Overall the model seems to be correctly understanding the situational context of the game.  It struggles to value positions and player talent well.  It very much struggles to understand the team strategy features ('number_of_pass_rushers').  Better features to explain player skill/value are needed, better features to explain team strategy are needed.  I think we may have hit the ceiling with the model as far as situational understanding.  The bulk of the model's predictive accuracy seems to be coming from that, and we will need to increase strength of the other features mentioned in order to increase the overall accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e403fb-6e7d-407d-83ce-29e3390c3bbf",
   "metadata": {},
   "source": [
    "### NEW FEATURE IDEAS\n",
    "\n",
    "- the model clearly understands situational context and basic rules of the game.  What is does not have, in a properly digestible way, is the information on the people playing the game.  We have tried overly specific features like avg_qb_epa by specific player, but model does not seem to interpret this well or derive any signal from it.  The issue is most likely that the model is not seeing the correlation between the qb's performance and the situation.\n",
    "- Therefore, we need to engineer features that explicitly tell the model how players perform in specific situations and circumstances.  For example, creating a metric (on a scale of 1-100) that grades a QB on how they performed against the blitz in the regular season preceding the playoff game in question.  If the model is given 'num_of_pass_rushers' and 'qb_value_vs_blitz' it can learn, \"this defense is in likely blitz formation, the qb is very good against the blitz, this play is likely to succeed\".  It will learn to derive a signal from the correlation of these two features.  This can be done with performance on certain downs, performance at certain points in the game (time-based), team rushing abilities against certain defense schemes, wide receiver corps performance against certain coverages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d702ed-9460-49bb-aade-c3c11c0b1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_seasons = list(range(2016, 2026))\n",
    "modern_seasons_extreme = list(range(2022, 2026))\n",
    "\n",
    "ftn_df = nfl.load_ftn_charting(modern_seasons_extreme).to_pandas()\n",
    "\n",
    "next_gen_df = nfl.load_nextgen_stats(modern_seasons).to_pandas()\n",
    "\n",
    "team_df = nfl.load_team_stats(modern_seasons).to_pandas()\n",
    "\n",
    "player_df = nfl.load_player_stats(modern_seasons).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81169ea8-b649-4e9d-8f9a-4a33f41c5b3c",
   "metadata": {},
   "source": [
    "### NEW FEATURES TO ENGINEER\n",
    "\n",
    "1. qb_blitz_grade: the QB's EPA on plays with > 4 pass rushers, in the preceding regular season\n",
    "2. off_motion_rate: aggregate of is_motion by posteam in ftn_data to show how much an offense uses pre-snap motion to confuse defense\n",
    "3. def_avg_box_count: aggregate n_defense_box by defteam in ftn_data to show percentage of plays defense loads the box with < 7\n",
    "4. def_blitz_rate: aggregate n_blitzers by def_team in ftn_data to show likely said team is to blitz (aggressiveness)\n",
    "5. qb_time_to_throw: avg_time_throw over preceding reg season from nextgen_stats to show how long qb typically holds ball\n",
    "6. qb_aggressiveness_grade: use aggressiveness in nextgen_stats to show how often in preceding reg season qb throws ball into tight windows\n",
    "7. qb_air_yards_intent: use avg_completed_air_yards vs avg_intended_air_yards (nextgen_stats) from preceding reg season to differentiate between check down tendency and gunslinger qb's\n",
    "8. wr1_wopr_season: use wopr of teams #1 wide receiver from player_stats for preceding reg season to identify if the offense has a dominant WR1\n",
    "9. off_avg_yac_per_catch: player_df(receiving_yards_after_catch / receptions) from preceding reg season to identify offenses that rely on scheme/playmakers vs those that rely on pure air yards\n",
    "10. def_pressure_rate_season: team_df((def_sacks + def_qb_hits) / attempts) to identify defensive pressure rate from preceding reg season\n",
    "11. red_zone_efficiency: team_df -> find TD conversion rates on red zone plays in preceding reg season"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993c777-ace1-4d2d-8251-0c6b08b150aa",
   "metadata": {},
   "source": [
    "#### QB_BLITZ_GRADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ddeee-a512-4bbf-b8e9-702d742bf978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_situational_qb_grades(df):\n",
    "    merged_df = reg_df.merge(\n",
    "        participation_df[['game_id', 'play_id', 'number_of_pass_rushers']],\n",
    "        left_on=['game_id', 'play_id'],\n",
    "        right_on=['game_id', 'play_id'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # filter to only regular season data\n",
    "    regular_df = merged_df[merged_df['season_type'] == 'REG'].copy()\n",
    "\n",
    "    # identify blitz play (pass rushers > 4)\n",
    "    regular_df = regular_df.dropna(subset=['number_of_pass_rushers'])\n",
    "    regular_df['is_blitz'] = (regular_df['number_of_pass_rushers'] > 4).astype(int)\n",
    "\n",
    "    # calculate entire regular season blitz epa per qb\n",
    "    blitz_df = regular_df[regular_df['is_blitz'] == 1].copy()\n",
    "\n",
    "    # group by season and QB to get mean epa for the whole year \n",
    "    season_stats = blitz_df.groupby(['season', 'passer_player_id', 'passer_player_name'])['qb_epa'].agg(\n",
    "        blitz_epa_season = 'mean',\n",
    "        play_count='count'\n",
    "    ).reset_index()\n",
    "\n",
    "    # enforce a min sample size to avoid outliers \n",
    "    season_stats = season_stats[season_stats['play_count'] >= 100].copy()\n",
    "\n",
    "    # map to 1-100 grade scale\n",
    "    season_stats['qb_blitz_grade'] = season_stats.groupby('season')['blitz_epa_season'].rank(pct=True) * 100\n",
    "\n",
    "    # create lookup table\n",
    "    return season_stats[['season', 'passer_player_id', 'passer_player_name', 'qb_blitz_grade']]\n",
    "\n",
    "qb_scouting_grades = get_situational_qb_grades(modern_only_df)\n",
    "\n",
    "modern_only_df = modern_only_df.merge(\n",
    "    qb_scouting_grades, on=['season', 'passer_player_id'], how='left')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5816be0-8823-4461-8710-e1d9e661b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_only_df['qb_blitz_grade'] = modern_only_df['qb_blitz_grade'].fillna(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049db4a1-50ea-4df5-b897-45db0ae9aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_only_df = modern_only_df.drop(columns=['qb_blitz_grade_x', 'qb_blitz_grade_y', 'passer_player_name_x', 'passer_player_name_y'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b8abf-8c53-4c5f-9a8e-b16326975392",
   "metadata": {},
   "source": [
    "#### NEW FEATURES DERIVED FROM FTN_CHARTING DATA \n",
    "\n",
    "1. def_blitz_rate\n",
    "2. def_box_avg\n",
    "3. def_light_box_rate\n",
    "4. off_motion_rate\n",
    "5. off_play_action_rate\n",
    "6. off_screen_rate\n",
    "7. off_rpo_rate\n",
    "\n",
    "Note: ftn_charting data on nflreadpy is only available for the seasons 2022-2025.  Our regular season play-by-play data currently encompasses the 2016-2025 date range.  Since we will not be able to capture the necessary data for seasons 2016-2021, we will impute the league averages for those seasons.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82577a-be4b-4411-ae1e-57f6f095ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ftn_season_stats(ftn_df, reg_df):\n",
    "    # merge ftn_df with reg_df on seasons 2022 to 2025 (impute the missing data later)\n",
    "    merged_df = ftn_df.merge(\n",
    "        reg_df[['game_id', 'play_id', 'posteam', 'defteam']],\n",
    "        left_on=['nflverse_game_id', 'nflverse_play_id'],\n",
    "        right_on=['game_id', 'play_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # def tendencies grouped by defteam\n",
    "    def_stats = merged_df.groupby(['season', 'defteam']).agg(\n",
    "        def_blitz_rate = ('n_blitzers', lambda x: (x > 0).mean()),\n",
    "        def_box_avg = ('n_defense_box', 'mean'),\n",
    "        def_light_box_rate = ('n_defense_box', lambda x: (x <= 6).mean())\n",
    "    ).reset_index()\n",
    "\n",
    "    # offensive identity grouped by posteam\n",
    "    off_stats = merged_df.groupby(['season', 'posteam']).agg(\n",
    "        off_motion_rate = ('is_motion', 'mean'),\n",
    "        off_play_action_rate = ('is_play_action', 'mean'),\n",
    "        off_screen_rate = ('is_screen_pass', 'mean'),\n",
    "        off_rpo_rate = ('is_rpo', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    return def_stats, off_stats\n",
    "\n",
    "# calculate stats for 2022-2025\n",
    "def_tendencies, off_identities = get_ftn_season_stats(ftn_df, reg_df)\n",
    "\n",
    "# merge into modern_only_df using left join to keep 2016-2021 plays \n",
    "modern_only_df = modern_only_df.merge(def_tendencies, on=['season', 'defteam'], how='left')\n",
    "modern_only_df = modern_only_df.merge(off_identities, on=['season', 'posteam'], how='left')\n",
    "\n",
    "# impute missing data for 2016-2021 seasons with league averages \n",
    "ftn_cols = [\n",
    "    'def_blitz_rate', 'def_box_avg', 'def_light_box_rate', 'off_motion_rate', 'off_play_action_rate',\n",
    "    'off_screen_rate', 'off_rpo_rate'\n",
    "]\n",
    "\n",
    "for col in ftn_cols:\n",
    "    # calculate the league average from the 2022-2025\n",
    "    recent_avg = modern_only_df[col].mean()\n",
    "    # fill NaNs with recent_avg\n",
    "    modern_only_df[col] = modern_only_df[col].fillna(recent_avg)\n",
    "\n",
    "# verify\n",
    "print(f'Data shape: {modern_only_df.shape}')\n",
    "print(f'NaN and data type summary: {modern_only_df.info()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7e812-2154-4151-b0fd-ac7c82fd3bc7",
   "metadata": {},
   "source": [
    "#### NEW FEATURES DERIVED FROM NEXTGEN_STATS \n",
    "\n",
    "1. avg_time_to_throw\n",
    "2. avg_intended_air_yards\n",
    "3. avg_aggressiveness\n",
    "4. avg_cpoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437a101-1478-45f9-bcd2-4691d35b784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngs_qb_style(ngs_df):\n",
    "    qb_df = next_gen_df[\n",
    "        (next_gen_df['season_type'] == 'REG') &\n",
    "        (next_gen_df['player_position'] == 'QB')\n",
    "    ].copy()\n",
    "\n",
    "    # aggregate from weekly to season, group by season and gsis_id\n",
    "    qb_style = qb_df.groupby(['season', 'player_gsis_id']).agg(\n",
    "        qb_time_to_throw=('avg_time_to_throw', 'mean'),\n",
    "        qb_intended_air_yards=('avg_intended_air_yards', 'mean'),\n",
    "        qb_aggressiveness=('aggressiveness', 'mean'),\n",
    "        qb_cpoe=('completion_percentage_above_expectation', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    return qb_style\n",
    "\n",
    "# generate features\n",
    "qb_style_stats = get_ngs_qb_style(next_gen_df)\n",
    "\n",
    "# merge into modern_only_df, match passer_player_id to player_gsis_id, use left join to keep all plays in modern_only_df\n",
    "modern_only_df = modern_only_df.merge(\n",
    "    qb_style_stats,\n",
    "    left_on=['season', 'passer_player_id'],\n",
    "    right_on=['season', 'player_gsis_id'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7876ea5-d434-442c-a3c5-5b84bedc64b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# impute any missing values with league average for that stat\n",
    "ngs_cols = ['qb_time_to_throw', 'qb_intended_air_yards', 'qb_aggressiveness', 'qb_cpoe']\n",
    "for col in ngs_cols:\n",
    "    # calculate the regular season mean for the column in question\n",
    "    season_means = modern_only_df.groupby('season')[col].transform('mean')\n",
    "    # fill NaNs with calculated mean for that col/season\n",
    "    modern_only_df[col] = modern_only_df[col].fillna(season_means)\n",
    "\n",
    "# verify\n",
    "print(f'Data shape: {modern_only_df.shape}')\n",
    "print(f'NaN Summary: {modern_only_df.info()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f8fe0-27ef-4e14-9321-5d2be43fe1de",
   "metadata": {},
   "source": [
    "#### NEW FEATURES DERIVED FROM PLAYER_DF\n",
    "\n",
    "1. off_alpha_wopr\n",
    "2. off_avg_yac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3e38a-acb7-472f-99a0-9bb3f954cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weapon_features(player_df):\n",
    "    weapons_df = player_df[\n",
    "        (player_df['season_type'] == 'REG') &\n",
    "        (player_df['position'].isin(['WR', 'TE', 'RB']))\n",
    "    ].copy()\n",
    "\n",
    "    # indentify the #1 target in the offense (alpha)\n",
    "    # for each team and season, find the max WOPR player\n",
    "    alpha_stats = weapons_df.groupby(['season', 'team'])['wopr'].max().reset_index()\n",
    "    alpha_stats.rename(columns={'wopr': 'off_alpha_wopr'}, inplace=True)\n",
    "\n",
    "    # identify explosiveness (YAC ability)\n",
    "    team_yac = weapons_df.groupby(['season', 'team']).agg(\n",
    "        total_yac=('receiving_yards_after_catch', 'sum'),\n",
    "        total_receptions=('receptions', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # avoid division by zero\n",
    "    team_yac = team_yac[team_yac['total_receptions'] > 0].copy()\n",
    "    team_yac['off_avg_yac'] = team_yac['total_yac'] / team_yac['total_receptions']\n",
    "\n",
    "    # merge the two features\n",
    "    weapon_features = alpha_stats.merge(\n",
    "        team_yac[['season', 'team', 'off_avg_yac']],\n",
    "        on=['season', 'team'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    return weapon_features\n",
    "\n",
    "# generate features\n",
    "weapon_stats = get_weapon_features(player_df)\n",
    "\n",
    "# merge into modern_only_df\n",
    "modern_only_df = modern_only_df.merge(\n",
    "    weapon_stats,\n",
    "    left_on=['season', 'posteam'],\n",
    "    right_on=['season', 'team'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# drop 'recent_team' column from the merge\n",
    "if 'team' in modern_only_df.columns:\n",
    "    modern_only_df = modern_only_df.drop(columns=['team'])\n",
    "\n",
    "# impute missing values with season-specific mean\n",
    "weapon_cols = ['off_alpha_wopr', 'off_avg_yac']\n",
    "\n",
    "for col in weapon_cols:\n",
    "    season_means = modern_only_df.groupby('season')[col].transform('mean')\n",
    "    modern_only_df[col] = modern_only_df[col].fillna(season_means)\n",
    "\n",
    "# verify\n",
    "print(f'Data Shape: {modern_only_df.shape}')\n",
    "print(f'Missing values summary: {modern_only_df.info()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11230d8c-26d4-49cd-8734-a71dd81bea47",
   "metadata": {},
   "source": [
    "#### NEW FEATURES DERIVED FROM TEAM_DF\n",
    "\n",
    "1. off_3rd_down_epa\n",
    "2. def_3rd_down_epa\n",
    "3. off_rz_td_rate\n",
    "4. def_rz_td_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd37f5-4403-4521-b7d9-6c1426ef3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_efficiency_stats(pbp_df):\n",
    "    pbp_reg = reg_df[reg_df['season_type'] == 'REG'].copy()\n",
    "\n",
    "    third_down_df = pbp_reg[pbp_reg['down'] == 3].copy()\n",
    "\n",
    "    off_3rd = third_down_df.groupby(['season', 'posteam'])['epa'].mean().reset_index()\n",
    "    off_3rd.rename(columns={'epa': 'off_3rd_down_epa'}, inplace=True)\n",
    "\n",
    "    def_3rd = third_down_df.groupby(['season', 'defteam'])['epa'].mean().reset_index()\n",
    "    def_3rd.rename(columns={'epa': 'def_3rd_down_epa'}, inplace=True)\n",
    "\n",
    "    rz_df = pbp_reg[pbp_reg['yardline_100'] <= 20].copy()\n",
    "\n",
    "    off_rz = rz_df.groupby(['season', 'posteam'])['touchdown'].mean().reset_index()\n",
    "    off_rz.rename(columns={'touchdown': 'off_rz_td_rate'}, inplace=True)\n",
    "\n",
    "    def_rz = rz_df.groupby(['season', 'defteam'])['touchdown'].mean().reset_index()\n",
    "    def_rz.rename(columns={'touchdown': 'def_rz_td_rate'}, inplace=True)\n",
    "\n",
    "    return off_3rd, def_3rd, off_rz, def_rz\n",
    "\n",
    "# Generate features\n",
    "off_3rd, def_3rd, off_rz, def_rz = get_team_efficiency_stats(reg_df)\n",
    "\n",
    "modern_only_df = modern_only_df.merge(off_3rd, on=['season', 'posteam'], how='left')\n",
    "modern_only_df = modern_only_df.merge(off_rz, on=['season', 'posteam'], how='left')\n",
    "\n",
    "modern_only_df = modern_only_df.merge(def_3rd, on=['season', 'defteam'], how='left')\n",
    "modern_only_df = modern_only_df.merge(def_rz, on=['season', 'defteam'], how='left')\n",
    "\n",
    "# impute missing values with season, league mean\n",
    "cols = ['off_3rd_down_epa', 'off_rz_td_rate', 'def_3rd_down_epa', 'def_rz_td_rate']\n",
    "\n",
    "for col in cols:\n",
    "    season_means = modern_only_df.groupby('season')[col].transform('mean')\n",
    "    modern_only_df[col] = modern_only_df[col].fillna(season_means)\n",
    "\n",
    "# Verify\n",
    "print(f'Data Shape: {modern_only_df.shape}')\n",
    "print(f'Missing Values Summary: {modern_only_df.info()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d95a22-dfdf-4aaf-a30a-bc2fb6b7e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_only_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f702c7-eeb4-44a2-b994-a4f06d24df7d",
   "metadata": {},
   "source": [
    "### PREP DATA FOR MODELING AGAIN AFTER NEW FEATURE ENGINEERING\n",
    "\n",
    "1. drop ID columns\n",
    "   - game_id, play_id, season, passer_play_id, passer_player_name, player_gsis_id, posteam, defteam\n",
    "2. drop redundant columns\n",
    "   - offense_personnel, offense_formation, personnel_group, personnel_clean, posteam_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a2091-4dad-4a22-a4a3-b1bdebff1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'game_id', 'play_id', 'season', 'passer_player_id', 'passer_player_name', 'player_gsis_id',\n",
    "    'posteam', 'defteam', 'offense_formation', 'offense_personnel', 'personnel_group', 'personnel_clean',\n",
    "    'posteam_type', 'success'\n",
    "]\n",
    "\n",
    "model_ready_df = modern_only_df.drop(columns=[c for c in cols_to_drop if c in modern_only_df.columns])\n",
    "\n",
    "print(f'Verify dataframe column data types: {model_ready_df.info()}')\n",
    "print(f'Data Shape: {model_ready_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a249c5-28b7-485e-9184-bc3ebfcf9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run feature correlation check, pre-training\n",
    "import numpy as np\n",
    "\n",
    "corr_matrix = model_ready_df.corr().abs()\n",
    "\n",
    "# check upper triangle of matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# find any features with correlation > 0.95\n",
    "high_corr = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "print(f'Features with > 0.95 Correlation: {high_corr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f99f7-f484-46b6-90e3-8f98ef1f355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create correlation matrix visualization\n",
    "\n",
    "high_corr_pairs = []\n",
    "\n",
    "for col in upper.columns:\n",
    "    rows = upper.index[upper[col] > 0.95].tolist()\n",
    "    for row in rows:\n",
    "        high_corr_pairs.append({\n",
    "            'Feature 1': row,\n",
    "            'Feature 2': col,\n",
    "            'Correlation': upper.loc[row, col]\n",
    "        })\n",
    "\n",
    "pairs_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False)\n",
    "print(f'Highly Correlated Pairs:\\n{pairs_df}')\n",
    "\n",
    "# Plot the matrix \n",
    "involved_features = list(set(pairs_df['Feature 1']) | set(pairs_df['Feature 2']))\n",
    "\n",
    "if involved_features:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    subset_corr = model_ready_df[involved_features].corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    sns.heatmap(subset_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "    plt.title('Correlation Matrix: Redundant Feature Clusters')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('\\nNo Correlations > 0.95 found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c49248-2d9f-4c50-b599-b888e54b8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'num_blockers', 'defenders_in_box', 'avg_def_epa']\n",
    "\n",
    "model_ready_df = modern_only_df.drop(columns=[c for c in cols_to_drop if c in modern_only_df.columns])\n",
    "\n",
    "print(f'Verify dataframe column data types: {model_ready_df.info()}')\n",
    "print(f'Data Shape: {model_ready_df.shape}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb9714-2648-466b-8e07-0fa82b19ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_drop = [\n",
    "    'avg_qb_epa', 'avg_rush_epa', 'avg_rec_epa', 'num_blockers', 'defenders_in_box', 'avg_def_epa',\n",
    "    'game_id', 'play_id', 'season', 'passer_player_id', 'passer_player_name', 'player_gsis_id',\n",
    "    'posteam', 'defteam', 'offense_formation', 'offense_personnel', 'personnel_group', 'personnel_clean',\n",
    "    'posteam_type', 'success'\n",
    "]\n",
    "\n",
    "model_ready_df = model_ready_df.drop(columns=[c for c in cols_to_drop if c in model_ready_df.columns])\n",
    "\n",
    "duplicate_cols = [c for c in model_ready_df.columns if c.endswith('_x') or c.endswith('_y')]\n",
    "if duplicate_cols:\n",
    "    print(f\"Dropping duplicate merge artifacts: {duplicate_cols}\")\n",
    "    model_ready_df = model_ready_df.drop(columns=duplicate_cols)\n",
    "\n",
    "\n",
    "print(f\"Old Column Count: 84 (or similar)\")\n",
    "print(f\"New Column Count: {model_ready_df.shape[1]}\")\n",
    "print(\"Columns Remaining:\", model_ready_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84df2c-85ba-45da-bedc-b81bd9ac6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ready_df = model_ready_df.drop(columns=['play_type_run'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919eb37-8883-40f8-8893-b563364dd320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back 'success' to model_ready_df\n",
    "model_ready_df['success'] = modern_only_df.loc[model_ready_df.index, 'success']\n",
    "\n",
    "model_ready_df['success'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d673669-552b-490c-8664-718ffcbc1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X = model_ready_df.drop(columns=['success'])\n",
    "y = model_ready_df['success']\n",
    "\n",
    "# convert bools to floats\n",
    "X = X.astype(float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_v1 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "model_v1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_v1.predict(X_test)\n",
    "\n",
    "print(f'Model V1 Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(model_v1, max_num_features=20, height=0.5, importance_type='weight', title='Top 20 Important Features')\n",
    "plt.show()\n",
    "\n",
    "explainer = shap.Explainer(model_v1, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('SHAP Summary')\n",
    "shap.summary_plot(shap_values, X_test, max_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22dd44-2d56-4265-b40b-53deb3956e34",
   "metadata": {},
   "source": [
    "### OBSERVATIONS ON MODEL V1:\n",
    "\n",
    "1. Recall for Class 1 (Success) is 0.52 which means the model is missing 58% of the successful plays\n",
    "2. Precision is 0.64 which is solid\n",
    "\n",
    "The model is still favoring the majority class (Failure) and being too conservative.  The feature set is not providing a strong enough signal on what determines a successful play result.  This could be because the large feature set (70 features) is injecting too much noise into the training.  \n",
    "\n",
    "Next Steps\n",
    "-----------\n",
    "\n",
    "1. Feature Selection:  Investigate which features are providing no value (importance) according to the model.  Drop the bottom ~25%\n",
    "2. If no significant increase in accuracy after feature selection, try hyperparameter tuning to fix recall\n",
    "3. Possibly try a linear baseline using Logistic Regression to see if a less complex model can provide a similar result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8e4aa-4b8a-4139-9493-8cf839903f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# identify weak features and drop them\n",
    "selection = SelectFromModel(model_v1, threshold='0.5*mean', prefit=True)\n",
    "X_train_selected = selection.transform(X_train)\n",
    "X_test_selected = selection.transform(X_test)\n",
    "\n",
    "selected_feat_mask = selection.get_support()\n",
    "selected_features = X.columns[selected_feat_mask]\n",
    "\n",
    "print(f'Original Feature Count: {X_train.shape[1]}')\n",
    "print(f'Selected Feature Count: {X_train_selected.shape[1]}')\n",
    "print(f'Dropped Features: {list(set(X.columns) - set(selected_features))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd94be-f478-4815-9dbc-f8d045b20694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'scale_pos_weight': [1, 1.2, 1.5]\n",
    "}\n",
    "\n",
    "xgb_tuned = xgb.XGBClassifier(random_state=42, n_jobs=1, eval_metric='logloss')\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_tuned,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_selected, y_train)\n",
    "\n",
    "print(f'\\nBest Parameters: {random_search.best_params_}')\n",
    "print(f'Best CV Accuracy: {random_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f05e45-ecc1-4bcc-ae61-2f7c1554b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test_selected)\n",
    "\n",
    "print(f'Best Model Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}')\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ac309-1fb4-49d1-a1ac-bbd500a3c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run less complex Logistic Regression model on selected features\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_selected, y_train)\n",
    "print(f'LogisiticRegression Baseline Accuracy: {lr.score(X_test_selected, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837c933-1f74-4785-98d5-bd27af0675d0",
   "metadata": {},
   "source": [
    "### OBSERVATIONS AFTER EXTENSIVE TUNING AND FEATURE ENGINEERING\n",
    "\n",
    "The model has likely plateaued at ~67% accuracy which is solid for an NFL prediction model working on EPA (play success).  With this in mind, it may be beneficial to investigate the model's errors (where the model had a high degree of confidence but was significantly wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d4f1b-745f-4b4a-b69c-128a3b110413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities for the test set\n",
    "y_probs = best_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# create analysis dataframe\n",
    "results_df = X_test.copy()\n",
    "results_df['actual_success'] = y_test\n",
    "results_df['predicted_success'] = y_pred_tuned\n",
    "results_df['prob_success'] = y_probs\n",
    "\n",
    "# add context from modern_only_df\n",
    "context_cols = ['season', 'posteam', 'defteam']\n",
    "results_df = results_df.join(modern_only_df[context_cols], how='left')\n",
    "\n",
    "# identify false positives\n",
    "false_positives = results_df[\n",
    "    (results_df['actual_success'] == 0) & \n",
    "    (results_df['prob_success'] > 0.80)\n",
    "].sort_values('prob_success', ascending=False)\n",
    "\n",
    "# identify false negatives\n",
    "false_negatives = results_df[\n",
    "    (results_df['actual_success'] == 1) & \n",
    "    (results_df['prob_success'] < 0.20)\n",
    "].sort_values('prob_success', ascending=True)\n",
    "\n",
    "\n",
    "print(f\"False Positives: {len(false_positives)}\")\n",
    "print(f\"False Negatives: {len(false_negatives)}\")\n",
    "\n",
    "print(\"\\n--- Top 3 False Positives\")\n",
    "for i, row in false_positives.head(3).iterrows():\n",
    "    print(f\"{row['posteam']} vs {row['defteam']} | Down: {row['down']} Distance: {row['ydstogo']}\")\n",
    "    print(f\"Model Confidence: {row['prob_success']:.1%}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Top 3 False Negatives\")\n",
    "for i, row in false_negatives.head(3).iterrows():\n",
    "    print(f\"{row['posteam']} vs {row['defteam']} | Down: {row['down']} Distance: {row['ydstogo']}\")\n",
    "    print(f\"Model Confidence: {row['prob_success']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae1612-4d89-48dd-9125-911253d2305e",
   "metadata": {},
   "source": [
    "### CALIBRATION TESTING OBSERVATIONS\n",
    "\n",
    "The model appears to be well calibrated.  The overwhelming amount of incorrect predictions are those where the model did not appear to have much confidence in its prediction.  Only 12 times did it have a probability of success outside of the 20-80 range and predict the result incorrectly.  And of those 12 times, the biggest misses were on situations where the likelihood for it to be correct seems reasonable.  For example, NO vs PHI the situation is 3rd and 1.  The model was predicted success with 83.5% confidence.  Most would view this as a play with high odds to end in a 1st down.  The play failed.  Another example, MIA vs KC the situation is 4th and 6.  The model predicted failure on this play with a probability of success of 3.7%.  Again, most analysts or viewers would align with this prediction.  The play resulted in success.  Essentially, the model understands what constitutes situations in an NFL game where success and failure are likely.  However, it is conservative in predicting success when it is unsure of the play result.  Adding stronger features could possibly provide the model with the added signal it needs to increase accuracy, but I think we may have achieved the accuracy score ceiling with the data available to us (the score has not budged from 66-67% throughout extensive feature engineering).  It is also possible that the inherent noise and variance of the data in an NFL game is near impossible to encapsulate in objective features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e12aa7-8f34-41dc-8308-371b6bcae6e5",
   "metadata": {},
   "source": [
    "### PROBABILITY HISTOGRAM\n",
    "\n",
    "1. Red -> Actual Failures\n",
    "2. Green -> Actual Successes\n",
    "\n",
    "We should most of our data clustered in the 0.40-0.60 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6278e0c-0911-42b1-8980-dad4f4a9756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data using y_probs and y_test\n",
    "viz_df = pd.DataFrame({\n",
    "    'Probability': y_probs,\n",
    "    'Outcome': y_test.map({1: 'Success', 0: 'Failure'})\n",
    "})\n",
    "\n",
    "# create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    data=viz_df,\n",
    "    x='Probability',\n",
    "    hue='Outcome',\n",
    "    element='step',\n",
    "    stat='percent',\n",
    "    common_norm=False,\n",
    "    bins=25,\n",
    "    palette={'Success': 'green', 'Failure': 'red'},\n",
    "    alpha=0.3\n",
    ")\n",
    "\n",
    "# add context lines\n",
    "plt.axvline(0.5, color='black', linestyle='--', alpha=0.5, label='Decision Threshold')\n",
    "\n",
    "plt.title('Model Confidence Distribution')\n",
    "plt.xlabel('Predicted Probability of Success')\n",
    "plt.ylabel('Percent of Plays')\n",
    "plt.xlim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7037224-f1d5-4a9c-bc99-32af3be5b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, ConfusionMatrixDisplay\n",
    "## create Confusion Matrix, ROC Curve and SHA Dependence visualizations for final model\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "#------------Confusion Matrix----------------\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Failure', 'Success'])\n",
    "disp.plot(cmap='Blues', ax=axes[0], colorbar=False)\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "#--------------ROC Curve--------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve\\n(Higher Arch = Better Model)')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "#--------------SHAP Dependence---------------\n",
    "# plot actual success rate by 'ydstogo' which the model consistently saw as a top important feature\n",
    "sns.regplot(\n",
    "    data=model_ready_df,\n",
    "    x='ydstogo',\n",
    "    y='success',\n",
    "    x_bins=15,\n",
    "    fit_reg=False,\n",
    "    ax=axes[2],\n",
    "    color='green',\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "axes[2].set_title('Actual Success Rate vs Yards To Go')\n",
    "axes[2].set_xlabel('Yards to Go')\n",
    "axes[2].set_ylabel('Observed Success Probability')\n",
    "axes[2].set_xlim(0, 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ac66d-ba35-48d7-a1ab-71ec8b70206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature importance plot for best_model\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print('Top 10 Features of Final Model')\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance_df['Feature'][:20][::-1], feature_importance_df['Importance'][:20][::-1], color='blue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance: Top 20')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fec834-c51e-4729-a29a-9414601686f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8398ceba-9c5c-4374-b3bd-ece76c3c0d04",
   "metadata": {},
   "source": [
    "### SAVE FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54855d-002f-400b-9e99-5b6fd8ffb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# define artifacts to save\n",
    "artifacts = {\n",
    "    'model': best_model,\n",
    "    'features': selected_features.tolist(),\n",
    "    'accuracy': accuracy_score(y_test, y_pred_tuned)\n",
    "}\n",
    "\n",
    "# save to file\n",
    "joblib.dump(artifacts, 'epa_succes_model_v1.pkl')\n",
    "print('Model saved')\n",
    "print(f'Saved with {len(selected_features)} features and {artifacts['accuracy']:.4f} accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98a769-7475-467e-8cc1-33ae4ce84b8b",
   "metadata": {},
   "source": [
    "### SAVE FINAL DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbcb34-4ab9-40bc-8ea1-ff31e2b6b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_only_df.to_parquet('data/enriched_plays_v1.parquet')\n",
    "print('Data saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35592bcc-8c8b-4eb4-a50b-0e844b948b89",
   "metadata": {},
   "source": [
    "# PHASE 1 SUMMARY: THE MICRO MODEL (BASELINE NFL GAME KNOWLEDGE)\n",
    "\n",
    "## OBJECTIVE\n",
    "To build a baseline model capable of predicting the success (positive EPA) of any given NFL play based solely on pre-snap information.\n",
    "\n",
    "## METHODOLOGY\n",
    "- **Data**: 13,000+ plays from post season NFL games from years 2016-2025\n",
    "- **Features**: Engineered 70+ signals including Scheme (motion, box counts), QB traits (CPOE, aggressiveness), and Situational Context (down, distance, score, weather)\n",
    "- **Model**: XGBoost Classifier tuned for probability calibration\n",
    "\n",
    "## RESULTS\n",
    "- **Accuracy**: ~66% (state-of-the-art for public EPA models usually tops out at ~70% due to the randomness of NFL games)\n",
    "- **Key Insight**: The model is highly calibrated and conservative.  It rarely predicts >80% success unless the situation is overwhelmingly favorable to the offense\n",
    "- **Top Predictive Features**:\n",
    "  1. 'qb_blitz_grade' (what is the QB's EPA against the blitz)\n",
    "  2. 'yards_to_goal_diff' (how close to the opponent's endzone is the offense)\n",
    "  3. 'possession_seconds' (how long has this offensive possession been in seconds)\n",
    "  4. 'active_rec_epa' (what is the preceding reg season epa of the active receiving corps for this play)\n",
    "  5. 'down' (what down is it)\n",
    "  6. 'posteam_timeouts_remaining' (how many timeouts does the offense have)\n",
    "  7. 'yardline_100' (where is the ball currently on the field)\n",
    "  8. 'active_qb_epa' (what is the reg season epa of the quarterback for this play)\n",
    "  9. 'ydstogo' (how many yards to obtain a 1st down)\n",
    "  10. 'game_seconds_remaining' (how much time is remaining in the game)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
